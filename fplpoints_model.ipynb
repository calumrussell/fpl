{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "fplpoints_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calumrussell/fpl/blob/master/fplpoints_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o00oPnXQlS6L"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xomhsk9tlS6a"
      },
      "source": [
        "teams = pd.read_csv(\"https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2019-20/teams.csv\")\n",
        "fixtures = pd.read_csv(\"https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2019-20/fixtures.csv\")\n",
        "players_raw = pd.read_csv(\"https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2019-20/players_raw.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyxH8la4lS6p"
      },
      "source": [
        "gws = []\n",
        "for i in range(1, 47):\n",
        "    df = pd.read_csv(f'https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2019-20/gws/gw{i}.csv')\n",
        "    gws.append(df)\n",
        "players = pd.concat(gws)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--hU0SLAlS6x"
      },
      "source": [
        "##Removing all the data we don't need\n",
        "players_filt = players[['name','value','fixture','opponent_team','total_points','minutes','transfers_balance','was_home']]\n",
        "fixtures_filt = fixtures[['id', 'team_a','team_h']]\n",
        "teams_filt = teams[['id', 'strength', 'short_name']]\n",
        "players_raw_filt = players_raw[['id','element_type']].astype('int64')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgAVD3kalS67",
        "outputId": "5550b615-b9a6-4c73-c9e4-f24f7add15e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "players_filt['player_id'] = players_filt['name'].apply(lambda x: x.split(\"_\")[2]).astype('int64')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xFcjT3slS7H"
      },
      "source": [
        "##Merging the fixtures onto players so we have information about the match and the player's team and opponent\n",
        "merged = players_filt.merge(fixtures_filt, how='left', left_on='fixture', right_on='id')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnjXZ0TPlS7P"
      },
      "source": [
        "##Merging position onto our players\n",
        "merged = merged.merge(players_raw_filt, how='left', left_on='player_id', right_on='id' )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDi3DxzKlS7V"
      },
      "source": [
        "import numpy as np\n",
        "##Create a row for the player's team\n",
        "merged['team'] = np.where(merged['opponent_team'] == merged['team_a'], merged['team_h'], merged['team_a'])\n",
        "##Create a row for player team difficulty\n",
        "merged = merged.merge(teams_filt, how='left', left_on='team', right_on='id')\n",
        "merged.rename(columns={'strength': 'team_diff', 'short_name': 'team_short'}, inplace=True)\n",
        "##Create a row for opponent team difficulty\n",
        "merged = merged.merge(teams_filt, how='left', left_on='opponent_team', right_on='id')\n",
        "merged.rename(columns={'strength': 'opp_diff', 'short_name': 'opp_short'}, inplace=True)\n",
        "##Create a row for difficulty difference\n",
        "merged['diff_diff'] = merged['team_diff'] - merged['opp_diff']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSemOrn3lS7f",
        "outputId": "37822469-3875-4b28-94d6-6f12bbcb9a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "merged.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>value</th>\n",
              "      <th>fixture</th>\n",
              "      <th>opponent_team</th>\n",
              "      <th>total_points</th>\n",
              "      <th>minutes</th>\n",
              "      <th>transfers_balance</th>\n",
              "      <th>was_home</th>\n",
              "      <th>player_id</th>\n",
              "      <th>id_x</th>\n",
              "      <th>team_a</th>\n",
              "      <th>team_h</th>\n",
              "      <th>id_y</th>\n",
              "      <th>element_type</th>\n",
              "      <th>team</th>\n",
              "      <th>id_x</th>\n",
              "      <th>team_diff</th>\n",
              "      <th>team_short</th>\n",
              "      <th>id_y</th>\n",
              "      <th>opp_diff</th>\n",
              "      <th>opp_short</th>\n",
              "      <th>diff_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aaron_Cresswell_376</td>\n",
              "      <td>50</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>376</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "      <td>376</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>WHU</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>MCI</td>\n",
              "      <td>-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aaron_Lennon_430</td>\n",
              "      <td>50</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>430</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>430</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>BUR</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>SOU</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aaron_Mooy_516</td>\n",
              "      <td>50</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>516</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>516</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>BHA</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>WAT</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aaron_Ramsdale_494</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>494</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>494</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>BOU</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>SHU</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aaron_Wan-Bissaka_122</td>\n",
              "      <td>55</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>122</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>MUN</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>CHE</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    name value fixture  ... opp_diff opp_short diff_diff\n",
              "0    Aaron_Cresswell_376    50       8  ...        5       MCI        -3\n",
              "1       Aaron_Lennon_430    50       3  ...        3       SOU         0\n",
              "2         Aaron_Mooy_516    50       7  ...        3       WAT        -1\n",
              "3     Aaron_Ramsdale_494    45       2  ...        3       SHU        -1\n",
              "4  Aaron_Wan-Bissaka_122    55       9  ...        4       CHE         0\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqRe5SblS7p"
      },
      "source": [
        "##Was_home to int value\n",
        "merged['is_home'] = merged['was_home'].apply(lambda x: 1 if x else 0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qzrGnwdlS7x"
      },
      "source": [
        "Here, we perform cleaning to help improve the accuracy of our results.\n",
        "\n",
        "First, we cut all entries off below zero, and add one to the points in every row. We do this because the data is heavily right skewed, and we need to take the log of points (and we can't have log(0)).\n",
        "\n",
        "We will be modelling results with simple regression, and if we take the histogram of total points (not included here to make this post shorter), we see that we have lots of values around zero, and a few very large values. Most regression models tend to struggle with these skewed distributions, an assumption of OLS is normally distributed errors/linear parameters, so we need to unskew our data. In simple numeric terms: if we have a points total of 5 and a points total of 25, the log difference between these two scores is ~1x as opposed to 5x on the normal scale. We will explain more about the model choice later.\n",
        "\n",
        "Second, we remove matches where players didn't play. If we included these results, we would get a misleading picture of model accuracy, and would be predicting something we with certainty: if a player doesn't play, they score zero points. An interesting alternative choice here is to remove matches where players played less than 90 minutes (i.e. didn't start). We explore this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV-IaYdolS7z"
      },
      "source": [
        "import numpy as np\n",
        "cleaned = merged.copy(deep=True)\n",
        "\n",
        "cleaned.loc[cleaned['total_points'] < 0, 'total_points'] = 0\n",
        "cleaned['total_points'] = (cleaned['total_points'] + 1).astype('float32')\n",
        "cleaned['log_points'] = np.log(cleaned['total_points'])\n",
        "\n",
        "cleaned = cleaned.drop(cleaned[cleaned.minutes == 0].index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCGLPXRRlS79"
      },
      "source": [
        "X = cleaned[['log_points']].astype('float32')\n",
        "y = pd.DataFrame()\n",
        "y['team_short'] = cleaned['team_short']\n",
        "y['opp_short'] = cleaned['opp_short']\n",
        "y = pd.get_dummies(y, columns=['team_short'])\n",
        "y = pd.get_dummies(y, columns=['opp_short'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTzgI3HTlS8G"
      },
      "source": [
        "Our first model looks at points scored as a function of team and opponent strength. The purpose of this model is to familiarize you with: how to build a model, and the output.\n",
        "\n",
        "The only input here is the identity of the teams. This may seem a bit odd but what we are actually modelling here are the strength of teams, in terms of log points, offensively and defensively.\n",
        "\n",
        "We include an intercept for this model but we do not do this for all models. In this case, an intercept makes sense: it represents the average points scored exclusive of team/opponent strengths. But, in other cases, including an intercept made it difficult to interpret the parameters, so we do not include it every time (this is likely due to multicollinearity amongst independent variables, I am sure that a skilled practitioner could tease these differences out but it was beyond me)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj3hVyFxlS8I",
        "outputId": "acd9dad3-1a84-4945-b192-6f0f324ea369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import statsmodels.api as sm\n",
        "y = sm.add_constant(y)\n",
        "model = sm.OLS(X, y).fit()\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:             log_points   R-squared:                       0.048\n",
            "Model:                            OLS   Adj. R-squared:                  0.044\n",
            "Method:                 Least Squares   F-statistic:                     13.58\n",
            "Date:                Thu, 01 Oct 2020   Prob (F-statistic):           8.33e-83\n",
            "Time:                        00:35:55   Log-Likelihood:                -9396.5\n",
            "No. Observations:               10313   AIC:                         1.887e+04\n",
            "Df Residuals:                   10274   BIC:                         1.915e+04\n",
            "Df Model:                          38                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==================================================================================\n",
            "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
            "----------------------------------------------------------------------------------\n",
            "const              1.0577      0.005    195.901      0.000       1.047       1.068\n",
            "team_short_ARS     0.0376      0.026      1.465      0.143      -0.013       0.088\n",
            "team_short_AVL    -0.0316      0.026     -1.221      0.222      -0.082       0.019\n",
            "team_short_BHA    -0.0085      0.026     -0.333      0.739      -0.059       0.042\n",
            "team_short_BOU    -0.0610      0.026     -2.344      0.019      -0.112      -0.010\n",
            "team_short_BUR     0.1159      0.027      4.317      0.000       0.063       0.169\n",
            "team_short_CHE     0.0662      0.026      2.560      0.010       0.016       0.117\n",
            "team_short_CRY     0.0141      0.026      0.538      0.591      -0.037       0.066\n",
            "team_short_EVE     0.0033      0.026      0.129      0.897      -0.047       0.054\n",
            "team_short_LEI     0.1508      0.026      5.806      0.000       0.100       0.202\n",
            "team_short_LIV     0.2323      0.026      9.069      0.000       0.182       0.283\n",
            "team_short_MCI     0.2024      0.026      7.857      0.000       0.152       0.253\n",
            "team_short_MUN     0.1111      0.026      4.311      0.000       0.061       0.162\n",
            "team_short_NEW     0.0369      0.026      1.430      0.153      -0.014       0.088\n",
            "team_short_NOR    -0.1271      0.026     -4.957      0.000      -0.177      -0.077\n",
            "team_short_SHU     0.0971      0.026      3.738      0.000       0.046       0.148\n",
            "team_short_SOU     0.0286      0.026      1.104      0.270      -0.022       0.080\n",
            "team_short_TOT     0.0629      0.026      2.428      0.015       0.012       0.114\n",
            "team_short_WAT    -0.0341      0.026     -1.316      0.188      -0.085       0.017\n",
            "team_short_WHU     0.0082      0.026      0.315      0.753      -0.043       0.059\n",
            "team_short_WOL     0.1526      0.026      5.793      0.000       0.101       0.204\n",
            "opp_short_ARS     -0.0026      0.026     -0.100      0.921      -0.054       0.048\n",
            "opp_short_AVL      0.1028      0.026      3.985      0.000       0.052       0.153\n",
            "opp_short_BHA      0.1160      0.026      4.493      0.000       0.065       0.167\n",
            "opp_short_BOU      0.1524      0.026      5.897      0.000       0.102       0.203\n",
            "opp_short_BUR      0.0855      0.026      3.285      0.001       0.034       0.136\n",
            "opp_short_CHE     -0.0068      0.026     -0.261      0.794      -0.058       0.044\n",
            "opp_short_CRY      0.1259      0.026      4.849      0.000       0.075       0.177\n",
            "opp_short_EVE      0.0953      0.026      3.673      0.000       0.044       0.146\n",
            "opp_short_LEI     -0.0461      0.026     -1.783      0.075      -0.097       0.005\n",
            "opp_short_LIV     -0.1287      0.026     -4.944      0.000      -0.180      -0.078\n",
            "opp_short_MCI     -0.1421      0.026     -5.505      0.000      -0.193      -0.091\n",
            "opp_short_MUN     -0.0419      0.026     -1.616      0.106      -0.093       0.009\n",
            "opp_short_NEW      0.1456      0.026      5.585      0.000       0.094       0.197\n",
            "opp_short_NOR      0.2277      0.026      8.831      0.000       0.177       0.278\n",
            "opp_short_SHU      0.0771      0.026      2.968      0.003       0.026       0.128\n",
            "opp_short_SOU      0.0508      0.026      1.963      0.050    6.65e-05       0.102\n",
            "opp_short_TOT     -0.0277      0.026     -1.064      0.287      -0.079       0.023\n",
            "opp_short_WAT      0.1689      0.026      6.467      0.000       0.118       0.220\n",
            "opp_short_WHU      0.1322      0.026      5.099      0.000       0.081       0.183\n",
            "opp_short_WOL     -0.0265      0.026     -1.026      0.305      -0.077       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      411.776   Durbin-Watson:                   2.025\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              460.180\n",
            "Skew:                           0.514   Prob(JB):                    1.18e-100\n",
            "Kurtosis:                       2.878   Cond. No.                     3.71e+15\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 8.23e-28. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoltnOSSlS8R"
      },
      "source": [
        "We won't look at model performance here. Again, this is a simple (but useful) starter but we will examine the meaning of our model summary.\n",
        "\n",
        "Players that were part of a strong side - such as Liverpool - scored more points, and players that faced a weak opponent - such as Norwich - also scored more points. Not surprising but there is a lot of information outside the extreme cases. One example that I noticed is that Arsenal has a surprisingly weak offence and a surprisingly strong defence.\n",
        "\n",
        "Also, this data is ex-post: we are using the outcomes of matches to tell us which teams are strongest. The model is informative, not predictive. A model could be built using trailing strength parameters but these models are often quite tricky to worth with: we would need to control for fixture difficult, and to lots of validation to work out how many matches we need to gain information. In this case though, this model is just informative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APSBStmslS8S"
      },
      "source": [
        "The next model is our benchmark model. All factors that show predictive ability are included: assume that we can have accurate team strength parameters and lineups, what kind of accuracy could we produce?\n",
        "\n",
        "A key decision was model choice: OLS with log points, GLM with log points, or some other GLM with raw points that somehow worked.\n",
        "\n",
        "In practice, there was little difference between OLS and GLM with log points. A Poisson GLM did outperform in some cases but this difference seemed marginal. The last option didn't work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL8Dz1zmlS8T"
      },
      "source": [
        "X = cleaned[['log_points']].astype('float64')\n",
        "y = cleaned[['value', 'is_home', 'diff_diff', 'element_type', 'minutes']].astype('float64')\n",
        "y['team_short'] = cleaned['team_short']\n",
        "y['opp_short'] = cleaned['opp_short']\n",
        "\n",
        "y = pd.get_dummies(y, columns=['is_home'])\n",
        "y = pd.get_dummies(y, columns=['team_short'])\n",
        "y = pd.get_dummies(y, columns=['opp_short'])\n",
        "y = pd.get_dummies(y, columns=['element_type'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndgYQ2jIlS8b",
        "outputId": "a7fe633a-46a3-4c7a-f22a-6b01b0b7d986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import statsmodels.api as sm\n",
        "model = sm.OLS(X, y).fit()\n",
        "print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:             log_points   R-squared:                       0.236\n",
            "Model:                            OLS   Adj. R-squared:                  0.233\n",
            "Method:                 Least Squares   F-statistic:                     72.11\n",
            "Date:                Thu, 01 Oct 2020   Prob (F-statistic):               0.00\n",
            "Time:                        00:35:55   Log-Likelihood:                -8260.8\n",
            "No. Observations:               10313   AIC:                         1.661e+04\n",
            "Df Residuals:                   10268   BIC:                         1.694e+04\n",
            "Df Model:                          44                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "value                0.0073      0.001     14.320      0.000       0.006       0.008\n",
            "diff_diff            0.0622      0.004     15.182      0.000       0.054       0.070\n",
            "minutes              0.0081      0.000     41.461      0.000       0.008       0.008\n",
            "is_home_0.0          0.0753      0.018      4.150      0.000       0.040       0.111\n",
            "is_home_1.0          0.1798      0.018      9.858      0.000       0.144       0.216\n",
            "team_short_ARS      -0.0686      0.023     -2.996      0.003      -0.114      -0.024\n",
            "team_short_AVL       0.0503      0.023      2.205      0.028       0.006       0.095\n",
            "team_short_BHA       0.0836      0.022      3.720      0.000       0.040       0.128\n",
            "team_short_BOU      -0.0129      0.023     -0.564      0.573      -0.058       0.032\n",
            "team_short_BUR       0.0633      0.024      2.620      0.009       0.016       0.111\n",
            "team_short_CHE      -0.0418      0.023     -1.813      0.070      -0.087       0.003\n",
            "team_short_CRY      -0.0050      0.024     -0.211      0.833      -0.051       0.041\n",
            "team_short_EVE      -0.0312      0.023     -1.336      0.181      -0.077       0.015\n",
            "team_short_LEI       0.1069      0.023      4.551      0.000       0.061       0.153\n",
            "team_short_LIV      -0.0109      0.023     -0.480      0.631      -0.055       0.034\n",
            "team_short_MCI      -0.0576      0.023     -2.496      0.013      -0.103      -0.012\n",
            "team_short_MUN       0.0073      0.023      0.317      0.752      -0.038       0.052\n",
            "team_short_NEW       0.0649      0.023      2.801      0.005       0.019       0.110\n",
            "team_short_NOR      -0.0403      0.022     -1.792      0.073      -0.084       0.004\n",
            "team_short_SHU       0.1173      0.023      5.011      0.000       0.071       0.163\n",
            "team_short_SOU       0.0312      0.023      1.338      0.181      -0.015       0.077\n",
            "team_short_TOT      -0.0855      0.024     -3.630      0.000      -0.132      -0.039\n",
            "team_short_WAT      -0.0314      0.023     -1.349      0.177      -0.077       0.014\n",
            "team_short_WHU       0.0590      0.023      2.562      0.010       0.014       0.104\n",
            "team_short_WOL       0.0566      0.023      2.416      0.016       0.011       0.103\n",
            "opp_short_ARS        0.0027      0.023      0.118      0.906      -0.043       0.048\n",
            "opp_short_AVL       -0.0058      0.023     -0.254      0.799      -0.050       0.039\n",
            "opp_short_BHA        0.0022      0.023      0.099      0.921      -0.042       0.047\n",
            "opp_short_BOU        0.0445      0.023      1.959      0.050   -2.63e-05       0.089\n",
            "opp_short_BUR        0.0282      0.023      1.206      0.228      -0.018       0.074\n",
            "opp_short_CHE        0.0058      0.023      0.252      0.801      -0.040       0.051\n",
            "opp_short_CRY        0.0662      0.023      2.840      0.005       0.021       0.112\n",
            "opp_short_EVE        0.0408      0.023      1.753      0.080      -0.005       0.086\n",
            "opp_short_LEI       -0.0938      0.023     -4.046      0.000      -0.139      -0.048\n",
            "opp_short_LIV       -0.0572      0.023     -2.532      0.011      -0.101      -0.013\n",
            "opp_short_MCI       -0.0617      0.022     -2.761      0.006      -0.106      -0.018\n",
            "opp_short_MUN       -0.0311      0.023     -1.340      0.180      -0.077       0.014\n",
            "opp_short_NEW        0.0889      0.023      3.801      0.000       0.043       0.135\n",
            "opp_short_NOR        0.1221      0.023      5.405      0.000       0.078       0.166\n",
            "opp_short_SHU        0.0209      0.023      0.896      0.370      -0.025       0.067\n",
            "opp_short_SOU       -0.0058      0.023     -0.251      0.802      -0.051       0.040\n",
            "opp_short_TOT       -0.0237      0.023     -1.018      0.309      -0.069       0.022\n",
            "opp_short_WAT        0.1082      0.023      4.615      0.000       0.062       0.154\n",
            "opp_short_WHU        0.0139      0.023      0.608      0.543      -0.031       0.058\n",
            "opp_short_WOL       -0.0101      0.023     -0.437      0.662      -0.055       0.035\n",
            "element_type_1.0     0.1332      0.018      7.232      0.000       0.097       0.169\n",
            "element_type_2.0    -0.0416      0.011     -3.776      0.000      -0.063      -0.020\n",
            "element_type_3.0     0.0555      0.012      4.569      0.000       0.032       0.079\n",
            "element_type_4.0     0.1080      0.018      5.979      0.000       0.073       0.143\n",
            "==============================================================================\n",
            "Omnibus:                      296.466   Durbin-Watson:                   2.020\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              322.251\n",
            "Skew:                           0.423   Prob(JB):                     1.06e-70\n",
            "Kurtosis:                       3.187   Cond. No.                     1.30e+18\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 5.32e-29. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNu0m1xlS8i"
      },
      "source": [
        "This model includes:\n",
        "* Player value before the match\n",
        "* The difference in the team difficulty i.e. the difficulty advantage over the opponent\n",
        "* Minutes played, this is kind of data leakage but assumes we can predict who will play, which isn't totally unreasonable\n",
        "* Whether the player's team is home or away (we include dummies so we can see the actual parameter value for both).\n",
        "* Dummies for the player's team and opponent's team\n",
        "* The player's position\n",
        "\n",
        "We are clearly having problems with multicollinearity here but this model is going purely for: let's just see what the limit on prediction is. \n",
        "\n",
        "In particular, the values for the home/away dummy, team/opponent dummies, and positions have lost all meaning.\n",
        "\n",
        "For example, the parameter value for Liverpool's team dummy is negative. Logically, being on Liverpool helps a player score more points...so what is happening?\n",
        "\n",
        "The likely source of the issue is correlation with one or more other factors in the model: team strength is correlated with another independent variable (or more than one).\n",
        "\n",
        "In this case, the culprit is likely to be player value (which is a very strong parameter): playing for Liverpool did help score more points but most of Liverpool's players had high prices, so our parameter estimate for team strength is reduced.\n",
        "\n",
        "To see this more clearly, look at the value of the dummy for Manchester City: even more negative, probably due to their players having high valuations with fairly poor results (note: we don't care about this, the aim of FPL is to maximise total points, not points per value).\n",
        "\n",
        "I did test this theory, with a model including just team/opponent dummies and value, and this relationship held...but this isn't conclusive. Hopefully, this section highlights the difficulty in interpreting model parameters when there is correlation between independent variables/multicollinearity (we don't do this here but it is possible although often difficult to remove these effects)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24PsUU2lS8j",
        "outputId": "b388a6b6-209c-43d5-dcc2-fc96c2467318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(model.aic, mean_squared_error(X, model.predict()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16611.581448981335 0.2905841727702081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q50E5OdClS8q"
      },
      "source": [
        "Multicollinearity doesn't change our predictions and this benchmark model performed okay: MSE of 0.29, this is log points though so this is 1.32 of \"real\" points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHymX051lS8s"
      },
      "source": [
        "only_90 = cleaned.drop(cleaned[cleaned.minutes != 90].index)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i-eE7sDlS8x"
      },
      "source": [
        "X = only_90[['log_points']].astype('float64')\n",
        "y = only_90[['value', 'is_home', 'diff_diff', 'element_type']].astype('float64')\n",
        "y['team_short'] = only_90['team_short']\n",
        "y['opp_short'] = only_90['opp_short']\n",
        "\n",
        "y = pd.get_dummies(y, columns=['is_home'])\n",
        "y = pd.get_dummies(y, columns=['team_short'])\n",
        "y = pd.get_dummies(y, columns=['opp_short'])\n",
        "y = pd.get_dummies(y, columns=['element_type'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irNIS8eKlS83"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "model = sm.OLS(X, y).fit()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYT0RmKylS8-",
        "outputId": "0b2e7a83-90a3-41cb-ad2c-af0a130e5cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.aic, mean_squared_error(X, model.predict()))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10779.017978437227 0.34405237393661975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBPmZo52lS9F"
      },
      "source": [
        "Removing those players who didn't start, we actually get a worse result in terms of mean squared error. We might expect the opposite here: surely, it would be easier to predict outcomes when all players were on the pitch for the same length of time. But, given that it is harder, it seems likely that the previous results in our benchmark model were due to the fact that most players in our sample didn't score many points: most starters in a match don't score many points, and most subs don't score many points either.\n",
        "\n",
        "We also got a lower AIC above, this is likely to due to features of the minutes variable. I didn't examine this but it is possibly correlated to other factors, again most likely value, in such a way that removing the minutes variable altogether improves the model quality (although not predictions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVnFNO93lS9G",
        "outputId": "bebd0933-8384-4503-80e6-bed201fac45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "df = pd.DataFrame({\"actual\":X['log_points'].values, \"predicted\": model.predict()})\n",
        "bins = np.linspace(df.actual.min(), df.actual.max(), 7)\n",
        "groups = df.groupby(np.digitize(df.actual, bins))\n",
        "groups.mean()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.070184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.177833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.149512</td>\n",
              "      <td>1.317539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.918399</td>\n",
              "      <td>1.346409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.346137</td>\n",
              "      <td>1.420521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.814387</td>\n",
              "      <td>1.557345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.218876</td>\n",
              "      <td>1.345473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     actual  predicted\n",
              "1  0.000000   1.070184\n",
              "2  0.693147   1.177833\n",
              "3  1.149512   1.317539\n",
              "4  1.918399   1.346409\n",
              "5  2.346137   1.420521\n",
              "6  2.814387   1.557345\n",
              "7  3.218876   1.345473"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBAVqjc8lS9O"
      },
      "source": [
        "We can represent this failure to predict higher values in the above table. We have binned our predictions, and taken the mean of each bin over the range of actual values.\n",
        "\n",
        "Our model is predicting values that do increase but which appear to be centered around 1.3-1.5. Nowhere near what we see in reality.\n",
        "\n",
        "To say this simply: it is easy to get a small-sounding error when the majority of your sample is below 5 points. You just always say: 1.3 points, and you will be close most of the time. Our model isn't doing this, but the error we have sounds more impressive than it is.\n",
        "\n",
        "Additionally, our sample still has a chunky mode around these low values. Our model performs well here but terribly everywhere else. Our attempts to unskew this data were not wholly successful.\n",
        "\n",
        "Statistical theory suggests that a general linear model using alternate parametric assumptions - for example, Poisson regression - will outperform OLS when we have errors that increase. But no model, that I tried, did actually outperform OLS. However, there other options.\n",
        "\n",
        "The most likely option would be to recognise that we are modelling a mixture of distributions. We could model this non-parameterically. Or we could split our model by position: either constructing a seperate model per position (if we had more detailed player-level stats) or a mixed effects model with position as fixed effects. \n",
        "\n",
        "An interesting alternative, in theory although it possibly wouldn't work in practice, would be a state space model. We model a normal match, we model a \"points haul\" match where a player scores lots of points, and then make our total points prediction on points in each prediction weighted by the probability of being in each state. In reality, the probability of a points haul isn't identically distributed between players so a position-based model would likely be a more worthwhile first iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSyqjKr9lS9Q"
      },
      "source": [
        "X = cleaned[['log_points']].astype('float64')\n",
        "y = cleaned[['value', 'is_home']].astype('float64')\n",
        "\n",
        "y = pd.get_dummies(y, columns=['is_home'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIVwkENPlS9U"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "model = sm.OLS(X, y).fit()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1nJLnglS9a",
        "outputId": "231b98ed-8f1d-4a55-d016-9c33fa47b965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(model.aic, mean_squared_error(X, model.predict()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18717.08990510335 0.3593148284219615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR4qYk3GlS9h"
      },
      "source": [
        "Reusing our dataset with substitute players and creating a very simple model: player value and whether the player is at home. MSE was 0.35 against 0.29 versus our benchmark model. In other words, the value of adding lots of information above just player value and home/away is 0.06 log points. Our AIC is higher, which also suggests that adding more parameters did improve accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzApP7jIlS9j"
      },
      "source": [
        "X = cleaned[['log_points']].astype('float64')\n",
        "y = cleaned[['value', 'is_home', 'diff_diff', 'element_type', 'team_diff', 'opp_diff', 'minutes']].astype('float64')\n",
        "\n",
        "##Team and opponent difficulty are actual ordinal values, and the scale itself doesn't have meaning\n",
        "y = pd.get_dummies(y, columns=['is_home'])\n",
        "y = pd.get_dummies(y, columns=['team_diff'])\n",
        "y = pd.get_dummies(y, columns=['opp_diff'])\n",
        "y = pd.get_dummies(y, columns=['element_type'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mapNmIWMlS9o"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "model = sm.OLS(X, y).fit()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kY0aCi7lS9u",
        "outputId": "940f7984-a2b6-4020-ded8-6fa62c88f3f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(model.aic, mean_squared_error(X, model.predict()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16710.25186098493 0.295203993712952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbL5scMHlS9z"
      },
      "source": [
        "We now propose a more realistic ex-ante model with less precise measures of team and opponent strength, and assuming some ability to predict who will play (proxied by the minutes variable).\n",
        "\n",
        "Our results are, surprisingly, most of the way to our benchmark model (which had an MSE of 0.29). But, somewhat interestingly given what we found with AIC dropping with minutes being excluded, this appears to be due largely to our minutes variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntFDznIdlS90"
      },
      "source": [
        "X = only_90[['log_points']].astype('float64')\n",
        "y = only_90[['value', 'is_home', 'diff_diff', 'element_type', 'team_diff', 'opp_diff']].astype('float64')\n",
        "\n",
        "##Team and opponent difficulty are actual ordinal values, and the scale itself doesn't have meaning\n",
        "y = pd.get_dummies(y, columns=['is_home'])\n",
        "y = pd.get_dummies(y, columns=['team_diff'])\n",
        "y = pd.get_dummies(y, columns=['opp_diff'])\n",
        "y = pd.get_dummies(y, columns=['element_type'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNTBnMnlS94"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "model = sm.OLS(X, y).fit()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNc8VA2YlS99",
        "outputId": "657e5b51-e3db-4696-eb89-6766a827428b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(model.aic, mean_squared_error(X, model.predict()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10852.34024666836 0.3519681429568843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7uHVYZllS-E"
      },
      "source": [
        "If we use our dataset that only has players who started and drop the minutes variable, we see a very marginal increase in MSE relative to our model that only had value and home/away: MSE of 0.351 vs 0.359."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNs8y1iFlS-F"
      },
      "source": [
        "**What conclusions can be draw?**\n",
        "\n",
        "1. Points scored is heavily skewed. Log transform improves model accuracy but there is still more improvement out there.\n",
        "2. Points scored is multi-modal. Other analysis I have performed suggest that points distribution varies by player position, a mixed effects model could be more appropriate or seperate models for each position.\n",
        "3. We have not tested the construction of a proper ex-ante team strength model but the very basic team strength model offered by FPL suggests that team strength does improve model accuracy but in a clearly bounded fashion. Other analysis I have performed suggests there are interesting correlations between offensive/defensive strength and position: in other words, some teams score points through defending/clean-sheets.\n",
        "4. Player value/home advantage clearly have predictive value.\n",
        "5. Player-level stats are likely crucial to model accuracy. Even our ex-post model with perfect estimates of team strength suggests that there is still significant amounts of variation that is distinct from team strength. This might be surprising: don't players at the top clubs score all the points? In the top 3, maybe but there is clearly stil more to this variable. Our last model has MSE of 1.4 points (non-log) but there is still a lot left to explain outside of low points outcomes."
      ]
    }
  ]
}